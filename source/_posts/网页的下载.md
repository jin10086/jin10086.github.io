---
title: 网页的下载
date: 2018-02-03 15:43:11
tags:
- 爬虫
categories:
- 爬虫
---
## 本文章属于[爬虫入门到精通系统教程](https://zhuanlan.zhihu.com/p/25296437)第四讲

在爬虫入门到精通第二讲中，我们了解了[HTTP协议](https://zhuanlan.zhihu.com/p/25301841)，那么我们现在使用这些协议来快速爬虫吧

## 本文的目标

当你看完本文后，你应该能爬取（几乎）任何的网页

## 使用chrome抓包

> 抓包（packet capture）就是将[网络传输](http://link.zhihu.com/?target=http%3A//baike.baidu.com/view/1542295.htm)发送与接收的[数据包](http://link.zhihu.com/?target=http%3A//baike.baidu.com/view/25880.htm)进行截获、重发、编辑、转存等操作，也用来检查网络安全。抓包也经常被用来进行数据截取等。

## 第一个案列：抓取[轮子哥的动态](https://www.zhihu.com/people/excited-vczh/activities)

1. 打开轮子哥动态这个网页
  {% asset_img 0.png  %}

2.  打开抓包工具
 - 点击F12打开开发者工具
 - 点击Network(或者网络)
 - 按F5刷新下页面（主要是让请求重发一次，这样就能抓到包了）
 - 应该会看到如下界面
 {% asset_img 1.png  %}

3. 找到我们需要的请求
 - 可以看到如下截图，里面有这么多的请求，那么到底哪一个才是我们需要的呢 ？
 {% asset_img 2.png  %}

 - **这边提供一个小技巧**
	  - 当你要抓的包是需要按F5刷新才出来的，一般我们需要的请求都在DOC里面(整个页面有刷新)
	  - 当你要抓的包是点击按钮"加载更多"(或者拖到页面最下面会自动加载的,整个页面并没有刷新）一般我们需要的请求都在XHR里面

 - 简单来讲就是如果整个页面没有刷新的话，那就是在XHR里面，否则在DOC里面

 - 因为本次抓包整个页面有刷新，所以，我们需要找的请求在DOC下面，可以看到只有一个请求
 {% asset_img 3.png  %}

4. 验证请求是对的
 - 有以下两种方法（基本上用1，因为比较快）
	 - 在我们要抓包的页面随便copy出几个字，在Respoinse中使用ctrl+f 查找，如果有找到，说明我们找到的是对的 （我查找的是"和微软粉丝谈"）
	 {% asset_img 4.png  %}
 	 - 把response中所有的内容复制到一个txt中，并改名为"#.html"(这里的#可以随便取)然后打开这个html，看看是否和我们要抓的一样
 	 - {% asset_img 5.png  %}

 - 如果发现要找的不对，那你可以打开下一个请求检查下

5. 模拟发送

 - 点击Headers

 - 可以看到请求的url是： https://www.zhihu.com/people/excited-vczh/activities

 - 方法是： GET

 - requests headers 是（下图中框出来的地方）
 {% asset_img 6.png  %}

 - 所以我们的代码应该是：

```
import requests

# 这里的headers就是我们上图框中的headers
request_headers = {        'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',   
    'Accept-Encoding':'gzip, deflate, sdch, br',
    'Accept-Language':'zh-CN,zh;q=0.8',
    'Cache-Control':'max-age=0',
    'Connection':'keep-alive',
    'Cookie':'',
    'Host':'www.zhihu.com',
    'Referer':'https://www.zhihu.com/',
    'Upgrade-Insecure-Requests':'1',
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'
}
#上图中的url
url = "https://www.zhihu.com/people/excited-vczh/activities"
# 上图中的请求方法（get）
z = requests.get(url,headers=request_headers)
print z.content

```



这段代码简单来说就是把 我们抓包看到的用程序来实现

**一个小总结**

**我们爬取一个网页的步骤可以分为如下：**

1.  **打开要爬取的网页**
2.  **打开开发者工具，并让请求重发一次（简单讲就是抓包）**
3.  **找到正确的请求**
4.  **用程序模拟发送**

## 第二个案列：点赞

**1.打开要爬取的网页**

我们打开 "[知乎 - 与世界分享你的知识、经验和见解](https://www.zhihu.com/)"

我们要点赞的回答是这个

{% asset_img 7.png  %}

**2.打开开发者工具，并让请求重发一次**

打开后"点击赞一下",可以看到有好多请求

{% asset_img 8.png  %}

**3.找到正确的请求**

我们一个一个的点开请求看，发现就一个有返回值，而且这个返回值没有意义，那么怎么确定这个就是我们要找的呢？

{% asset_img 9.png  %}

我们可以点击Headers，看一下发送的参数

vote_up 很明显，就是点赞的意思。所以这个应该就是我们要找的。

{% asset_img 10.png  %}

这边说一下，右边"Headers,Preview,Response,Cookies,Timing"是什么意思

我们经常要看的有，headers 和 preview

headers 里面我们都有介绍过（请求头，返回头）

preview和response里面的内容是相同的（preview里面的内容格式化了，输出的好看一些），里面的内容是html返回值

cookies 里面是cookie的值，只不过分成了key value的形式

Timing基本用不上,所以也不介绍了（想了解的话可以自己百度...）

{% asset_img 11.png  %}

4.用程序模拟发送

我们把headers全部copy，

url也和上面一样

参数也是对的

请求方法是post

但是发现最终返回http code 400,这是为什么呢？

{% asset_img 12.png  %}

让我们留到后面讲解~


## 最后再次总结一下

看完本篇文章后，你应该要

*   能学会抓包

最后大家可以抓一下知乎登录的包哦~

小提示：当你要抓的网页是会自动跳转的话，那么你需要选中“proserve log”

意思是不要在页面重新加载后清除log（抓知乎登录的包会用到）

{% asset_img 13.png  %}

